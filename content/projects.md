---
title: "Projects"
draft: false
---

## ResNet Reshaper: Reshaping Residual Networks for Resource-Efficient Inference on FPGAs
Residual networks (ResNets) employ skip connections in their networks---reusing activations from previous layers---to improve training convergence, but these skip connections create challenges for hardware implementations of ResNets. In particular, they are problematic for inference accelerators on resource-limited platforms because they introduce a bottleneck during inference: the layers whose activations are reused in future layers must wait for this reuse to occur before accepting new input and continuing to compute. To avoid this issue, extra buffers can be used to store these activations; however, this forces higher utilization of on-chip/off-chip memories, requiring larger memory bandwidth, and potentially additional control logic. ResNet Reshaper modifies the ResNet’s skip connections to make them more hardware friendly with minimal to no loss in network accuracy. We modify the ResNet gradually and dynamically as it is trained via knowledge distillation to recover accuracy lost by the modifications. With these changes, ResNet Reshaper decreases a traditional ResNet’s resource utilization by up to 23% for BRAMs and 25% for both FFs and LUTs on an FPGA. 

## Maximizing Channel Capacity in Time-to-Digital Converters
Side-channel leakage poses a major security threat in multi-tenant environments. In FPGA systems, one tenant can instantiate a voltage fluctuation sensor that measures minute changes in the power distribution network (PDN) and infer information about co-tenant computation and data. In this project, we present the Tunable Dual-Polarity Time-to-Digital Converter (TDC)---a voltage fluctuation sensor with three dynamically tuneable parameters: the sample duration, sample clock phase, and sample clock frequency. The sensor captures both rising and falling transition polarities which provides additional information about the target computation.

Parameter selection, dynamic tuning, and noise reduction are essential for mitigating extremely nonlinear behaviors within TDC sensors, extracting information about a co-tenant computation, and improving cross-board generalization. We measure the impact of these techniques in a multi-tenant scenario where the attacker aims to determine the type of computation (AES, PRESENT, Ring-Oscillator, Arithmetic-Heavy, and No-Sensor) as well as microarchitectural implementation (Microblaze, ORCA, PicoRV32, and CortexM3) of a co-tenant. This serves as a necessary precursor for exploits in a virtualized FPGA environment, where an attacker must identify a co-located application before performing an attack. Our results show that 13-way classification accuracy on 5-board, leave-one-out cross-validation, can be improved by 2.5X, from 32% to 80% and that noise reduction techniques reduce the interquartile range of cross-validation loss by 5.8X.

## Evaluating Achievable Latency and Cost: SSD Latency Predictors
[Workshop paper][1]

Cutting tail latencies at the millisecond level in internet services to achieve good response times in data-parallel applications is possible by integrating MittOS, an OS/data center interface. Typically, MittOS analyzes white-box information of the internals of devices such as SSD's and decides if a given server can "fast reject" a service request. But commercial SSD's have a black-box design, so MittOS researchers have developed machine learning models to determine if requests to commercial SSD's can be rejected or not. When run on CPUs, however, these models cannot predict in the time it takes an SSD to fully process a request, defeating MittOS's fast-rejecting abilities. We demonstrate that ASICs such as the Efficient Inference Engine (EIE) accelerate the prediction times of these MittOS models well within the time it takes an SSD to complete a request at minimal cost, cutting SSD tail latencies. EIE achieves 2.01 us inference latency while incurring minimal area costs (20.4 mm^2) and power costs (0.29 W). We show that integrating machine learning into the critical path of operating systems becomes cost-efficient and within reason.

[1]: /accml_2020.pdf
[2]: https://arxiv.org/abs/1912.13179